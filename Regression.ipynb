{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import numpy as np\n",
    "from torch.nn import ReLU, LeakyReLU\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Simple model with one hidden layer\n",
    "from src.models.regression import MLERegression\n",
    "from src.metrics.regression import  RootMeanSquaredError, PCIP, MPIW\n",
    "# Six UCI datasets: concrete, power, energy, wine, yacht, housing\n",
    "from src.data.regression import UCI\n",
    "from src.commons.pyro_training import to_bayesian_model\n",
    "from src.commons.utils import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sizes = {\n",
    "    'concrete': 7,\n",
    "    'power': 3,\n",
    "    'energy': 8,\n",
    "    'wine': 10,\n",
    "    'yacht': 5,\n",
    "    'housing': 12\n",
    "}\n",
    "\n",
    "relu = ReLU()\n",
    "leaky = LeakyReLU(negative_slope = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference example: http://pyro.ai/examples/bayesian_regression.html\n",
    "\n",
    "def training(svi, train_loader, epoch, device):\n",
    "    loss = 0\n",
    "    for idx, (X, y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        step_loss = svi.step(X, y)\n",
    "        loss += step_loss\n",
    "        batch_index = (epoch + 1) * len(train_loader) + idx\n",
    "    return loss\n",
    "\n",
    "def evaluation(predictive, dataloader, metrics, device):\n",
    "    for idx, (X, y) in enumerate(dataloader):\n",
    "        if not np.isfinite(np.array(X)).all():\n",
    "            print(\"WARNING: INF IN DATA\")\n",
    "            X[X == float(\"-INF\")] = 0\n",
    "            X[X == float(\"INF\")] = 0\n",
    "            \n",
    "        y = y.to(device)\n",
    "        out = predictive(X.to(device))[\"obs\"].T\n",
    "        for metric in metrics.values():\n",
    "            metric.update(out, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model,\n",
    "    guide,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    svi,\n",
    "    epochs,\n",
    "    num_samples,\n",
    "    metrics,\n",
    "    device,\n",
    "):\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        loss = training(svi, train_loader, e, device)\n",
    "#         print(f\"Loss: {loss}\")\n",
    "        \n",
    "        predictive = Predictive(model, guide=guide, num_samples=num_samples, return_sites=(\"obs\",))\n",
    "        evaluation(predictive, valid_loader, metrics, device)\n",
    "        \n",
    "        print(\"\\nEpoch:\", e)\n",
    "        for metric in metrics:\n",
    "            print(f\"{metric}: {metrics[metric].compute().cpu()}\")\n",
    "            metrics[metric].reset()\n",
    "            \n",
    "    return model, guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_training(\n",
    "    seed,\n",
    "    max_epochs,\n",
    "    num_samples,\n",
    "    batch_size,\n",
    "    task,\n",
    "    device,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    sigma_bound,\n",
    "    param_mean,\n",
    "    param_std,\n",
    "    hidden_size,\n",
    "    activation\n",
    "):\n",
    "    seed_everything(seed)\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    net = MLERegression(activation=activation, in_size=input_sizes[task], hidden_size=hidden_size, out_size=1)\n",
    "# the model is converted using code in src.models.bnn, in particular BNNRegression class defining bayesian model\n",
    "    model = to_bayesian_model(net, param_mean, param_std, device=device, sigma_bound=sigma_bound)\n",
    "    svi = SVI(model.model, model.guide, optimizer, loss=criterion)\n",
    "    \n",
    "    dataloader_args = {\n",
    "      \"num_workers\": 4,\n",
    "      \"pin_memory\": True,\n",
    "      \"persistent_workers\": True\n",
    "    }\n",
    "    \n",
    "    uci = UCI(task, train_batch_size=batch_size, test_batch_size=batch_size, train_ratio=0.8, validation_ratio=0.2, dataloader_args=dataloader_args)\n",
    "    train_loader = uci.train_dataloader()\n",
    "    valid_loader = uci.validation_dataloader()\n",
    "    \n",
    "    metrics = {\n",
    "    \"RMSE\": RootMeanSquaredError(input_type=\"samples\"),\n",
    "    \"PCIP\": PCIP(input_type=\"none\", percentile=50),\n",
    "    \"MPIW\": MPIW(input_type=\"none\", percentile=50)\n",
    "    }\n",
    "\n",
    "    for metric in metrics.values():\n",
    "        metric.set_device(device)\n",
    "        \n",
    "    return train_loop(model.model, model.guide, train_loader, valid_loader, svi, max_epochs, num_samples, metrics, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"seed\": np.random.randint(10000),\n",
    "    \"max_epochs\": 500,\n",
    "    \"num_samples\": 100,\n",
    "    \"batch_size\": 132,\n",
    "    \"task\": 'housing',\n",
    "#     \"task\": 'concrete',\n",
    "    \"task\": 'power',\n",
    "    \"device\": torch.device(\"cuda:0\"),\n",
    "\n",
    "#     \"optimizer\": pyro.optim.Adam({\"lr\": 1e-5 ,\"betas\": [0.9, 0.999], \"eps\":1e-4}),\n",
    "    \"optimizer\": pyro.optim.Adam({\"lr\": 1e-2}),\n",
    "#     \"optimizer\": pyro.optim.SGD({\"lr\": 1e-8}),\n",
    "    \"criterion\": Trace_ELBO(),\n",
    "\n",
    "    # The models output is Normal(mean, std) where std has prior Uniform(0, sigma_bound)\n",
    "    \"sigma_bound\": 5.0,\n",
    "\n",
    "    # mean and std of bayesian model priors\n",
    "    \"param_mean\": 0.0,\n",
    "    \"param_std\": 1.0,\n",
    "\n",
    "    \"hidden_size\": 50,\n",
    "    \"activation\": relu\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is normalized so RMSE of naive model is ~1. \n",
    "\n",
    "PCIP metric is adjusted for percentile so for example for 80% confidence interval value 5 means that 85% of datapoint fall into the interval and value -3 means that 77% of the datapoints fall into the interval. In other words, the closer the PCIP metric is to 0 the better.\n",
    "\n",
    "Here we can see that the model starts with relatively reasonable PCIP but as the model trains it goes up to 18 indicating that 98% of datapoints fall into 80% confidence interval (despite the fact that RMSE goes down so the model converges). \n",
    "\n",
    "By changing param_std, sigma_bound, learning_rate, batch_size or the shape of the net one can affect the course of training so that the initial PCIP can be negative/positive and increase/decrease throughout training. But I didn't manage to set the parameters in such a way that it converges to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape\n",
      "torch.Size([7654, 3])\n",
      "torch.Size([7654])\n",
      "Test data shape\n",
      "torch.Size([1914, 3])\n",
      "torch.Size([1914])\n",
      "NAIVE RMSE: 0.9996809450431293\n",
      "\n",
      "Epoch: 0\n",
      "RMSE: 0.4473884701728821\n",
      "PCIP: 41.895423889160156\n",
      "MPIW: 1.7906368970870972\n",
      "\n",
      "Epoch: 1\n",
      "RMSE: 0.5076503157615662\n",
      "PCIP: 38.69281005859375\n",
      "MPIW: 1.585242748260498\n",
      "\n",
      "Epoch: 2\n",
      "RMSE: 0.41440704464912415\n",
      "PCIP: 38.932464599609375\n",
      "MPIW: 1.500036597251892\n",
      "\n",
      "Epoch: 3\n",
      "RMSE: 0.4539216458797455\n",
      "PCIP: 39.65686798095703\n",
      "MPIW: 1.5613961219787598\n",
      "\n",
      "Epoch: 4\n",
      "RMSE: 0.3456208109855652\n",
      "PCIP: 40.418296813964844\n",
      "MPIW: 1.5067243576049805\n",
      "\n",
      "Epoch: 5\n",
      "RMSE: 0.3187048137187958\n",
      "PCIP: 40.34858703613281\n",
      "MPIW: 1.4362764358520508\n",
      "\n",
      "Epoch: 6\n",
      "RMSE: 0.33088061213493347\n",
      "PCIP: 40.80298614501953\n",
      "MPIW: 1.4135652780532837\n",
      "\n",
      "Epoch: 7\n",
      "RMSE: 0.35690540075302124\n",
      "PCIP: 40.30229187011719\n",
      "MPIW: 1.3733047246932983\n",
      "\n",
      "Epoch: 8\n",
      "RMSE: 0.4047638475894928\n",
      "PCIP: 39.970947265625\n",
      "MPIW: 1.3506795167922974\n",
      "\n",
      "Epoch: 9\n",
      "RMSE: 0.3228450119495392\n",
      "PCIP: 39.8431396484375\n",
      "MPIW: 1.3177143335342407\n",
      "\n",
      "Epoch: 10\n",
      "RMSE: 0.3598555624485016\n",
      "PCIP: 39.786094665527344\n",
      "MPIW: 1.3045454025268555\n",
      "\n",
      "Epoch: 11\n",
      "RMSE: 0.3351118862628937\n",
      "PCIP: 39.19390106201172\n",
      "MPIW: 1.2772712707519531\n",
      "\n",
      "Epoch: 12\n",
      "RMSE: 0.3049570322036743\n",
      "PCIP: 38.82855987548828\n",
      "MPIW: 1.2480807304382324\n",
      "\n",
      "Epoch: 13\n",
      "RMSE: 0.38113418221473694\n",
      "PCIP: 38.641456604003906\n",
      "MPIW: 1.2393879890441895\n",
      "\n",
      "Epoch: 14\n",
      "RMSE: 0.2934313416481018\n",
      "PCIP: 38.63616943359375\n",
      "MPIW: 1.2206151485443115\n",
      "\n",
      "Epoch: 15\n",
      "RMSE: 0.3027653396129608\n",
      "PCIP: 38.41094970703125\n",
      "MPIW: 1.2039333581924438\n",
      "\n",
      "Epoch: 16\n",
      "RMSE: 0.3064712882041931\n",
      "PCIP: 38.32756805419922\n",
      "MPIW: 1.187400460243225\n",
      "\n",
      "Epoch: 17\n",
      "RMSE: 0.3045038878917694\n",
      "PCIP: 38.2135009765625\n",
      "MPIW: 1.1754347085952759\n",
      "\n",
      "Epoch: 18\n",
      "RMSE: 0.33114904165267944\n",
      "PCIP: 38.20777130126953\n",
      "MPIW: 1.1662170886993408\n",
      "\n",
      "Epoch: 19\n",
      "RMSE: 0.4191151559352875\n",
      "PCIP: 38.052284240722656\n",
      "MPIW: 1.1675738096237183\n",
      "\n",
      "Epoch: 20\n",
      "RMSE: 0.3219250738620758\n",
      "PCIP: 37.89293670654297\n",
      "MPIW: 1.156755805015564\n",
      "\n",
      "Epoch: 21\n",
      "RMSE: 0.337205708026886\n",
      "PCIP: 37.68865203857422\n",
      "MPIW: 1.1468772888183594\n",
      "\n",
      "Epoch: 22\n",
      "RMSE: 0.3013007938861847\n",
      "PCIP: 37.34867858886719\n",
      "MPIW: 1.1306471824645996\n",
      "\n",
      "Epoch: 23\n",
      "RMSE: 0.3002781569957733\n",
      "PCIP: 37.118736267089844\n",
      "MPIW: 1.117130994796753\n",
      "\n",
      "Epoch: 24\n",
      "RMSE: 0.4663480520248413\n",
      "PCIP: 37.00653839111328\n",
      "MPIW: 1.1113064289093018\n",
      "\n",
      "Epoch: 25\n",
      "RMSE: 0.3112819194793701\n",
      "PCIP: 36.704376220703125\n",
      "MPIW: 1.1003458499908447\n",
      "\n",
      "Epoch: 26\n",
      "RMSE: 0.46912604570388794\n",
      "PCIP: 36.061485290527344\n",
      "MPIW: 1.0954004526138306\n",
      "\n",
      "Epoch: 27\n",
      "RMSE: 0.305940717458725\n",
      "PCIP: 35.845008850097656\n",
      "MPIW: 1.0844236612319946\n",
      "\n",
      "Epoch: 28\n",
      "RMSE: 0.40777820348739624\n",
      "PCIP: 35.83727264404297\n",
      "MPIW: 1.0806612968444824\n",
      "\n",
      "Epoch: 29\n",
      "RMSE: 0.4327191412448883\n",
      "PCIP: 35.60784149169922\n",
      "MPIW: 1.076198935508728\n",
      "\n",
      "Epoch: 30\n",
      "RMSE: 0.5081272721290588\n",
      "PCIP: 35.32574462890625\n",
      "MPIW: 1.069298267364502\n",
      "\n",
      "Epoch: 31\n",
      "RMSE: 0.6593464016914368\n",
      "PCIP: 35.11029815673828\n",
      "MPIW: 1.0612226724624634\n",
      "\n",
      "Epoch: 32\n",
      "RMSE: 0.5121474266052246\n",
      "PCIP: 34.96533966064453\n",
      "MPIW: 1.0545415878295898\n",
      "\n",
      "Epoch: 33\n",
      "RMSE: 0.6190049648284912\n",
      "PCIP: 34.815460205078125\n",
      "MPIW: 1.0494719743728638\n",
      "\n",
      "Epoch: 34\n",
      "RMSE: 0.3492821455001831\n",
      "PCIP: 34.57329559326172\n",
      "MPIW: 1.0423591136932373\n",
      "\n",
      "Epoch: 35\n",
      "RMSE: 0.4649985432624817\n",
      "PCIP: 34.10675048828125\n",
      "MPIW: 1.0355232954025269\n"
     ]
    }
   ],
   "source": [
    "bayesian_training(**param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
