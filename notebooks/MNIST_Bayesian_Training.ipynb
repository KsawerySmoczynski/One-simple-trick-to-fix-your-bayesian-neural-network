{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-V134Lz2UoUh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6VNkohR0UoUl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from IPython import display\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from scipy.misc import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YCuAp5SjUoUo"
   },
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AMKODk-QUoUm"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QtUU2rjlUoUn"
   },
   "outputs": [],
   "source": [
    "class FCNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.fc1(x)\n",
    "        if self.activation == 'relu':\n",
    "            output = F.relu(output)\n",
    "        elif self.activation == 'leaky':\n",
    "            output = F.leaky_relu(output, negative_slope=0.5)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown activation:\", self.activation)\n",
    "        output = self.out(output)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TCd0CcmaUoUq"
   },
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    scale_mult = 100.\n",
    "    fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight).cuda(), scale=scale_mult*torch.ones_like(net.fc1.weight).cuda())\n",
    "    fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias).cuda(), scale=scale_mult*torch.ones_like(net.fc1.bias).cuda())\n",
    "    \n",
    "    outw_prior = Normal(loc=torch.zeros_like(net.out.weight).cuda(), scale=scale_mult*torch.ones_like(net.out.weight).cuda())\n",
    "    outb_prior = Normal(loc=torch.zeros_like(net.out.bias).cuda(), scale=scale_mult*torch.ones_like(net.out.bias).cuda())\n",
    "    \n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,  'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "    lhat = log_softmax(lifted_reg_model(x_data))\n",
    "    \n",
    "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RCMVoceUUoUr"
   },
   "outputs": [],
   "source": [
    "def guide(x_data, y_data):\n",
    "    \n",
    "    # First layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(net.fc1.weight).cuda()\n",
    "    fc1w_sigma = torch.randn_like(net.fc1.weight).cuda()\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    # First layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(net.fc1.bias).cuda()\n",
    "    fc1b_sigma = torch.randn_like(net.fc1.bias).cuda()\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    # Output layer weight distribution priors\n",
    "    outw_mu = torch.randn_like(net.out.weight).cuda()\n",
    "    outw_sigma = torch.randn_like(net.out.weight).cuda()\n",
    "    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n",
    "    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n",
    "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)\n",
    "    # Output layer bias distribution priors\n",
    "    outb_mu = torch.randn_like(net.out.bias).cuda()\n",
    "    outb_sigma = torch.randn_like(net.out.bias).cuda()\n",
    "    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n",
    "    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n",
    "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    \n",
    "    return lifted_module()\n",
    "\n",
    "def predict(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return torch.argmax(mean, dim=1)\n",
    "\n",
    "def calculate_test_acc():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j, data in inmemory_test_loader:\n",
    "        images, labels = data\n",
    "        predicted = predict(images.view(-1,28*28).cuda())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.cuda()).sum().item()\n",
    "    print(\"accuracy: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UmY8KicpUoUs",
    "outputId": "2bc551ba-9d18-4cb5-9885-6ed41ed2421d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######\n",
      "leaky\n",
      "######\n",
      "accuracy: 9 %\n",
      "Epoch  1  Loss  2074.2865249093375\n",
      "accuracy: 13 %\n",
      "Epoch  2  Loss  2127.0972722943625\n",
      "accuracy: 22 %\n",
      "Epoch  3  Loss  1997.337711575826\n",
      "accuracy: 26 %\n",
      "Epoch  4  Loss  2040.4098923190434\n",
      "accuracy: 29 %\n",
      "Epoch  5  Loss  1999.6886578385036\n",
      "accuracy: 34 %\n",
      "Epoch  6  Loss  1948.39791528066\n",
      "accuracy: 40 %\n",
      "Epoch  7  Loss  1989.8549863306682\n",
      "accuracy: 43 %\n",
      "Epoch  8  Loss  1886.6735256385803\n",
      "accuracy: 46 %\n",
      "Epoch  9  Loss  1843.7492190249761\n",
      "accuracy: 51 %\n",
      "Epoch  10  Loss  1903.878629989624\n",
      "accuracy: 52 %\n",
      "Epoch  11  Loss  1822.203501121203\n",
      "accuracy: 58 %\n",
      "Epoch  12  Loss  1777.9595104646683\n",
      "accuracy: 57 %\n",
      "Epoch  13  Loss  1912.4692790937424\n",
      "accuracy: 56 %\n",
      "Epoch  14  Loss  1911.9795507828394\n",
      "accuracy: 57 %\n",
      "Epoch  15  Loss  1823.3979569292069\n",
      "accuracy: 62 %\n",
      "Epoch  16  Loss  1792.8115548149744\n",
      "accuracy: 59 %\n",
      "Epoch  17  Loss  1807.897892964681\n",
      "accuracy: 63 %\n",
      "Epoch  18  Loss  1856.3359571107228\n",
      "accuracy: 61 %\n",
      "Epoch  19  Loss  1823.2330675284068\n",
      "accuracy: 65 %\n",
      "Epoch  20  Loss  1749.3408809026082\n",
      "accuracy: 65 %\n",
      "Epoch  21  Loss  1800.747553764979\n",
      "accuracy: 66 %\n",
      "Epoch  22  Loss  1764.22451338768\n",
      "accuracy: 68 %\n",
      "Epoch  23  Loss  1764.9088443851472\n",
      "accuracy: 67 %\n",
      "Epoch  24  Loss  1747.7267850653332\n",
      "accuracy: 69 %\n",
      "Epoch  25  Loss  1734.0467708206177\n",
      "accuracy: 70 %\n",
      "Epoch  26  Loss  1703.764421075185\n",
      "accuracy: 70 %\n",
      "Epoch  27  Loss  1698.3231724150976\n",
      "accuracy: 70 %\n",
      "Epoch  28  Loss  1721.0960913467406\n",
      "accuracy: 70 %\n",
      "Epoch  29  Loss  1698.2452222919464\n",
      "accuracy: 70 %\n",
      "Epoch  30  Loss  1755.1154556337992\n",
      "accuracy: 72 %\n",
      "Epoch  31  Loss  1750.9348798656463\n",
      "accuracy: 72 %\n",
      "Epoch  32  Loss  1694.7534873819352\n",
      "accuracy: 72 %\n",
      "Epoch  33  Loss  1687.0535131104787\n",
      "accuracy: 72 %\n",
      "Epoch  34  Loss  1712.755352911949\n",
      "accuracy: 72 %\n",
      "Epoch  35  Loss  1682.168540395101\n",
      "accuracy: 73 %\n",
      "Epoch  36  Loss  1719.9520833762488\n",
      "accuracy: 72 %\n",
      "Epoch  37  Loss  1701.3282169389724\n",
      "accuracy: 74 %\n",
      "Epoch  38  Loss  1657.2256194353104\n",
      "accuracy: 74 %\n",
      "Epoch  39  Loss  1660.199074734052\n",
      "accuracy: 75 %\n",
      "Epoch  40  Loss  1739.4571290588378\n",
      "accuracy: 75 %\n",
      "Epoch  41  Loss  1673.142626229922\n",
      "accuracy: 76 %\n",
      "Epoch  42  Loss  1691.9968494240443\n",
      "accuracy: 75 %\n",
      "Epoch  43  Loss  1670.5307466904321\n",
      "accuracy: 77 %\n",
      "Epoch  44  Loss  1649.02390030543\n",
      "accuracy: 76 %\n",
      "Epoch  45  Loss  1686.8847592671711\n",
      "accuracy: 76 %\n",
      "Epoch  46  Loss  1705.5509323422114\n",
      "accuracy: 76 %\n",
      "Epoch  47  Loss  1658.4512249978384\n",
      "accuracy: 76 %\n",
      "Epoch  48  Loss  1661.9427292553585\n",
      "accuracy: 76 %\n",
      "Epoch  49  Loss  1673.3724509270985\n",
      "accuracy: 77 %\n",
      "Epoch  50  Loss  1642.9996362320583\n",
      "accuracy: 77 %\n",
      "Epoch  51  Loss  1659.3271729866663\n",
      "accuracy: 77 %\n",
      "Epoch  52  Loss  1690.0446953964233\n",
      "accuracy: 75 %\n",
      "Epoch  53  Loss  1656.6529038619994\n",
      "accuracy: 76 %\n",
      "Epoch  54  Loss  1677.8529563474656\n",
      "accuracy: 76 %\n",
      "Epoch  55  Loss  1652.6497606690725\n",
      "accuracy: 77 %\n",
      "Epoch  56  Loss  1654.2244572353363\n",
      "accuracy: 77 %\n",
      "Epoch  57  Loss  1641.9189377085368\n",
      "accuracy: 78 %\n",
      "Epoch  58  Loss  1647.2347665580114\n",
      "accuracy: 78 %\n",
      "Epoch  59  Loss  1625.5374701388678\n",
      "accuracy: 79 %\n",
      "Epoch  60  Loss  1695.4885639222464\n",
      "accuracy: 77 %\n",
      "Epoch  61  Loss  1632.4951331710815\n",
      "accuracy: 78 %\n",
      "Epoch  62  Loss  1662.8895806153616\n",
      "accuracy: 78 %\n",
      "Epoch  63  Loss  1654.1364247576396\n",
      "accuracy: 78 %\n",
      "Epoch  64  Loss  1639.0214548222225\n",
      "accuracy: 78 %\n",
      "Epoch  65  Loss  1612.2744569746653\n",
      "accuracy: 78 %\n",
      "Epoch  66  Loss  1638.9385583750407\n",
      "accuracy: 78 %\n",
      "Epoch  67  Loss  1688.3175599956512\n",
      "accuracy: 78 %\n",
      "Epoch  68  Loss  1637.8749441687266\n",
      "accuracy: 79 %\n",
      "Epoch  69  Loss  1647.245394390424\n",
      "accuracy: 79 %\n",
      "Epoch  70  Loss  1652.1829029607773\n",
      "accuracy: 79 %\n",
      "Epoch  71  Loss  1613.6064860741296\n",
      "accuracy: 79 %\n",
      "Epoch  72  Loss  1639.6169365549088\n",
      "accuracy: 79 %\n",
      "Epoch  73  Loss  1613.324166437785\n",
      "accuracy: 79 %\n",
      "Epoch  74  Loss  1608.1569574308396\n",
      "accuracy: 78 %\n",
      "Epoch  75  Loss  1611.6672352266312\n",
      "accuracy: 79 %\n",
      "Epoch  76  Loss  1625.6306401046118\n",
      "accuracy: 79 %\n",
      "Epoch  77  Loss  1648.5864390261968\n",
      "accuracy: 79 %\n",
      "Epoch  78  Loss  1621.1743483018874\n",
      "accuracy: 77 %\n",
      "Epoch  79  Loss  1632.1583457899094\n",
      "accuracy: 80 %\n",
      "Epoch  80  Loss  1643.4579530461629\n",
      "accuracy: 79 %\n",
      "Epoch  81  Loss  1654.7974360720316\n",
      "accuracy: 79 %\n",
      "Epoch  82  Loss  1622.1907450962067\n",
      "accuracy: 79 %\n",
      "Epoch  83  Loss  1655.1464834022522\n",
      "accuracy: 79 %\n",
      "Epoch  84  Loss  1615.3675925286611\n",
      "accuracy: 79 %\n",
      "Epoch  85  Loss  1636.0719062582652\n",
      "accuracy: 80 %\n",
      "Epoch  86  Loss  1616.6744792763393\n",
      "accuracy: 80 %\n",
      "Epoch  87  Loss  1645.7615678373973\n",
      "accuracy: 80 %\n",
      "Epoch  88  Loss  1608.2656948661804\n",
      "accuracy: 80 %\n",
      "Epoch  89  Loss  1616.0763624016445\n",
      "accuracy: 80 %\n",
      "Epoch  90  Loss  1621.1717533429464\n",
      "accuracy: 81 %\n",
      "Epoch  91  Loss  1618.9860422356924\n",
      "accuracy: 80 %\n",
      "Epoch  92  Loss  1632.063363219897\n",
      "accuracy: 80 %\n",
      "Epoch  93  Loss  1616.7266503095627\n",
      "accuracy: 81 %\n",
      "Epoch  94  Loss  1651.3412545569738\n",
      "accuracy: 81 %\n",
      "Epoch  95  Loss  1617.061803267797\n",
      "accuracy: 81 %\n",
      "Epoch  96  Loss  1601.1238058964411\n",
      "accuracy: 81 %\n",
      "Epoch  97  Loss  1616.6336472495398\n",
      "accuracy: 81 %\n",
      "Epoch  98  Loss  1613.7968614975612\n",
      "accuracy: 81 %\n",
      "Epoch  99  Loss  1630.0929523468017\n",
      "accuracy: 81 %\n",
      "Epoch  100  Loss  1605.665438841184\n",
      "accuracy: 80 %\n",
      "Epoch  101  Loss  1618.6685151688257\n",
      "accuracy: 79 %\n",
      "accuracy: 80 %\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 101\n",
    "num_samples = 100\n",
    "hidden_size = 128\n",
    "batch_size = 512\n",
    "train_limit = 6000\n",
    "\n",
    "# activation = 'relu'\n",
    "activation = 'leaky'\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "train_dataset.train_data = train_dataset.train_data[:train_limit]\n",
    "train_dataset.train_labels = train_dataset.train_labels[:train_limit]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8192, shuffle=True)\n",
    "inmemory_iter = [ a for a in enumerate(train_loader, 1) ]\n",
    "inmemory_test_loader = [ a for a in enumerate(test_loader) ]\n",
    "\n",
    "# for _ in range(5):\n",
    "#     for activation in ['relu', 'leaky']:\n",
    "#     # for activation in ['leaky', 'relu']:\n",
    "print(\"######\")\n",
    "print(activation)\n",
    "print(\"######\")\n",
    "net = FCNNet(28*28, hidden_size, 10, activation=activation)\n",
    "net.cuda()\n",
    "log_softmax = nn.LogSoftmax(dim=1).cuda()\n",
    "softplus = torch.nn.Softplus().cuda()\n",
    "\n",
    "# optim = Adam({\"lr\": 0.001})\n",
    "optim = SGD({\"lr\": 0.00005})\n",
    "\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())\n",
    "calculate_test_acc()\n",
    "for j in range(1, num_iterations + 1):\n",
    "    loss = 0\n",
    "    for batch_id, data in inmemory_iter:\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss += svi.step(data[0].view(-1,28*28).cuda(), data[1].cuda())\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = loss / normalizer_train\n",
    "    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)\n",
    "    if j % 1 == 0:\n",
    "        calculate_test_acc()\n",
    "calculate_test_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_limit 600\n",
    "# r: accuracy: 77 %\n",
    "# l: accuracy: 79 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######\n",
    "# leaky\n",
    "# ######\n",
    "# accuracy: 9 %\n",
    "# Epoch  1  Loss  7781.167767016236\n",
    "# accuracy: 88 %\n",
    "# Epoch  2  Loss  7536.092252284758\n",
    "# accuracy: 90 %\n",
    "# Epoch  3  Loss  7492.473983689086\n",
    "# accuracy: 91 %\n",
    "# Epoch  4  Loss  7472.4932125878495\n",
    "# accuracy: 91 %\n",
    "# Epoch  5  Loss  7450.396791742754\n",
    "# accuracy: 91 %\n",
    "# Epoch  6  Loss  7433.21803365206\n",
    "# accuracy: 92 %\n",
    "# Epoch  7  Loss  7423.984343831134\n",
    "# accuracy: 92 %\n",
    "# Epoch  8  Loss  7412.521513465866\n",
    "# accuracy: 92 %\n",
    "# Epoch  9  Loss  7399.077827001738\n",
    "# accuracy: 92 %\n",
    "# Epoch  10  Loss  7388.452180169885\n",
    "# accuracy: 92 %\n",
    "# Epoch  11  Loss  7383.5393468670845\n",
    "# accuracy: 92 %\n",
    "# Epoch  12  Loss  7367.353721424278\n",
    "# accuracy: 92 %\n",
    "# Epoch  13  Loss  7367.780766725215\n",
    "# accuracy: 92 %\n",
    "# Epoch  14  Loss  7352.903488879625\n",
    "# accuracy: 92 %\n",
    "# Epoch  15  Loss  7344.9674231809295\n",
    "# accuracy: 92 %\n",
    "# Epoch  16  Loss  7340.157877805623\n",
    "# accuracy: 92 %\n",
    "# Epoch  17  Loss  7332.449975951751\n",
    "# accuracy: 92 %\n",
    "# Epoch  18  Loss  7321.8990278904275\n",
    "# accuracy: 92 %\n",
    "# Epoch  19  Loss  7321.809581075191\n",
    "# accuracy: 92 %\n",
    "# Epoch  20  Loss  7311.237704139233\n",
    "# accuracy: 92 %\n",
    "# Epoch  21  Loss  7303.856645090119\n",
    "# accuracy: 92 %\n",
    "# Epoch  22  Loss  7295.705256912406\n",
    "# accuracy: 93 %\n",
    "# Epoch  23  Loss  7289.659198091658\n",
    "# accuracy: 92 %\n",
    "# Epoch  24  Loss  7284.563497493155\n",
    "# accuracy: 93 %\n",
    "# Epoch  25  Loss  7279.538372230721\n",
    "# accuracy: 93 %\n",
    "# Epoch  26  Loss  7272.080593576233\n",
    "# accuracy: 93 %\n",
    "# Epoch  27  Loss  7267.945162123632\n",
    "# accuracy: 92 %\n",
    "# Epoch  28  Loss  7261.418912718852\n",
    "# accuracy: 92 %\n",
    "# Epoch  29  Loss  7254.923328325033\n",
    "# accuracy: 92 %\n",
    "# Epoch  30  Loss  7246.238408061688\n",
    "# accuracy: 92 %\n",
    "# Epoch  31  Loss  7240.580533446948\n",
    "# accuracy: 93 %\n",
    "# accuracy: 93 %\n",
    "\n",
    "# ######\n",
    "# relu\n",
    "# ######\n",
    "# accuracy: 8 %\n",
    "# Epoch  1  Loss  7795.6394361015\n",
    "# accuracy: 88 %\n",
    "# Epoch  2  Loss  7476.434200631706\n",
    "# accuracy: 90 %\n",
    "# Epoch  3  Loss  7429.284371277976\n",
    "# accuracy: 91 %\n",
    "# Epoch  4  Loss  7399.794071721951\n",
    "# accuracy: 92 %\n",
    "# Epoch  5  Loss  7374.095147988057\n",
    "# accuracy: 93 %\n",
    "# Epoch  6  Loss  7360.183345735367\n",
    "# accuracy: 93 %\n",
    "# Epoch  7  Loss  7343.454786724019\n",
    "# accuracy: 93 %\n",
    "# Epoch  8  Loss  7331.558122437573\n",
    "# accuracy: 93 %\n",
    "# Epoch  9  Loss  7319.605682273396\n",
    "# accuracy: 94 %\n",
    "# Epoch  10  Loss  7310.955456221739\n",
    "# accuracy: 94 %\n",
    "# Epoch  11  Loss  7300.144362474227\n",
    "# accuracy: 94 %\n",
    "# Epoch  12  Loss  7292.083888504974\n",
    "# accuracy: 94 %\n",
    "# Epoch  13  Loss  7282.3227097644485\n",
    "# accuracy: 94 %\n",
    "# Epoch  14  Loss  7276.746759878715\n",
    "# accuracy: 94 %\n",
    "# Epoch  15  Loss  7267.70032061774\n",
    "# accuracy: 95 %\n",
    "# Epoch  16  Loss  7261.3901898210925\n",
    "# accuracy: 95 %\n",
    "# Epoch  17  Loss  7254.557094400669\n",
    "# accuracy: 95 %\n",
    "# Epoch  18  Loss  7248.980283724729\n",
    "# accuracy: 95 %\n",
    "# Epoch  19  Loss  7242.114744297274\n",
    "# accuracy: 95 %\n",
    "# Epoch  20  Loss  7235.464850984685\n",
    "# accuracy: 95 %\n",
    "# Epoch  21  Loss  7229.462998536452\n",
    "# accuracy: 95 %\n",
    "# Epoch  22  Loss  7224.141231817587\n",
    "# accuracy: 95 %\n",
    "# Epoch  23  Loss  7217.477503020286\n",
    "# accuracy: 95 %\n",
    "# Epoch  24  Loss  7212.017617628423\n",
    "# accuracy: 95 %\n",
    "# Epoch  25  Loss  7206.526013750982\n",
    "# accuracy: 95 %\n",
    "# Epoch  26  Loss  7200.673278092035\n",
    "# accuracy: 95 %\n",
    "# Epoch  27  Loss  7195.646281444009\n",
    "# accuracy: 96 %\n",
    "# Epoch  28  Loss  7189.906175079672\n",
    "# accuracy: 96 %\n",
    "# Epoch  29  Loss  7185.586503403401\n",
    "# accuracy: 96 %\n",
    "# Epoch  30  Loss  7179.328694003868\n",
    "# accuracy: 96 %\n",
    "# Epoch  31  Loss  7174.049581344334\n",
    "# accuracy: 96 %\n",
    "# accuracy: 96 %"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P7 - MNIST - Bayesian - Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyro-old",
   "language": "python",
   "name": "pyro-old"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
